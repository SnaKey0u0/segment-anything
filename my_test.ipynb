{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sam import SamPredictor, sam_model_registry\n",
    "from sam.utils.transforms import ResizeLongestSide\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b_01ec64.pth\")\n",
    "image_encoder = sam.image_encoder\n",
    "prompt_encoder = sam.prompt_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "text = clip.tokenize([\"brain\"]).to(device)\n",
    "text_features = clip_model.encode_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json,os\n",
    "# import numpy as np\n",
    "# from monai.transforms import (\n",
    "#     AsDiscrete,\n",
    "#     Compose,\n",
    "#     CropForegroundd,\n",
    "#     LoadImaged,\n",
    "#     Orientationd,\n",
    "#     RandFlipd,\n",
    "#     RandCropByPosNegLabeld,\n",
    "#     RandShiftIntensityd,\n",
    "#     ScaleIntensityRanged,\n",
    "#     Spacingd,\n",
    "#     RandRotate90d,\n",
    "#     EnsureTyped,\n",
    "#     SpatialCrop,\n",
    "#     AddChanneld,\n",
    "#     Transform,\n",
    "#     ResizeWithPadOrCropd,\n",
    "#     Lambda,\n",
    "# )\n",
    "# from monai.data import (\n",
    "#     ThreadDataLoader,\n",
    "#     CacheDataset,\n",
    "#     load_decathlon_datalist,\n",
    "#     decollate_batch,\n",
    "#     set_track_meta,\n",
    "#     DataLoader,\n",
    "# )\n",
    "\n",
    "# def get_evaluation_transform(spacing):\n",
    "#     return Compose(\n",
    "#         [\n",
    "#             LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=None),\n",
    "#             AddChanneld(keys=[\"image\", \"label\"]),\n",
    "#             ScaleIntensityRanged(\n",
    "#                 keys=[\"image\"], a_min=-1024, a_max=3071, b_min=0.0, b_max=255.0, clip=True\n",
    "#             ),\n",
    "#             CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "#             Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "#             Spacingd(\n",
    "#                 keys=[\"image\", \"label\"],\n",
    "#                 pixdim=(spacing[0], spacing[1], spacing[2]),\n",
    "#                 mode=(\"bilinear\", \"nearest\"),\n",
    "#             ),\n",
    "#             EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=True),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "# def custom_load_decathlon_datalist(json_data_path, index):\n",
    "#     with open(json_data_path, 'r', encoding='utf-8') as json_file:\n",
    "#         json_data = json_file.read()\n",
    "#     parsed_data = json.loads(json_data)\n",
    "#     target = parsed_data[index]\n",
    "#     # 路徑+去掉.變成完整路徑\n",
    "#     for i in range(len(target)):\n",
    "#         target[i]['image'] = data_dir + target[i]['image'][1:]\n",
    "#         target[i]['label'] = data_dir + target[i]['label'][1:]\n",
    "\n",
    "#     return target\n",
    "\n",
    "# # image數量\n",
    "# organ_num = {\n",
    "#     3:131,\n",
    "#     6:63,\n",
    "#     7:281,\n",
    "#     8:303,\n",
    "#     9:41,\n",
    "#     10:126,\n",
    "# }\n",
    "# spacings = {\n",
    "#     2: [1.25, 1.25, 1.37],\n",
    "#     3: [0.7676, 0.7676, 1],\n",
    "#     6: [0.79, 0.79, 1.24],\n",
    "#     7: [0.8, 0.8, 2.5],\n",
    "#     8: [0.8, 0.8, 1.5],\n",
    "#     9: [0.78, 0.78, 1.6],\n",
    "#     10: [0.78, 0.78, 3],\n",
    "# }\n",
    "# data_dir =\"D:\\\\SAM\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# task_list = [3,6,7,8,9,10] #3,6,7,8,9,10\n",
    "# for task in task_list:\n",
    "#     datasets = os.path.join(data_dir, f\"dataset_{task}.json\")\n",
    "\n",
    "#     # 得到data path list\n",
    "#     datalist = custom_load_decathlon_datalist(datasets, \"training\")\n",
    "\n",
    "#     split_index = organ_num[task]* 4 // 5\n",
    "#     segment1 = datalist[0:7]\n",
    "#     segment2 = datalist[split_index:min(split_index+7, organ_num[task])]\n",
    "#     # 连接两个段\n",
    "#     eval_list = segment1 + segment2\n",
    "#     print('eval_list', eval_list, len(eval_list))\n",
    "\n",
    "#     dataset = CacheDataset(\n",
    "#         data = eval_list,\n",
    "#         transform = get_evaluation_transform(spacings[task]),\n",
    "#         cache_num = 24,\n",
    "#         cache_rate = 1.0,\n",
    "#         num_workers = 8,\n",
    "#     )   \n",
    "\n",
    "#     # split_index = len(datalist) * 4 // 5\n",
    "\n",
    "#     for i in range(len(dataset)):\n",
    "#         image, label = dataset[i]['image'], dataset[i]['label']\n",
    "        \n",
    "#         # 獲取圖像和標籤的形狀\n",
    "#         # print(type(image))\n",
    "#         # print(image.shape)\n",
    "#         _, D, H, W = image.shape\n",
    "        \n",
    "#         # 遍歷每一個slice\n",
    "#         for d in range(D):\n",
    "#             # 獲取2D slice\n",
    "#             image_slice = image[0, d, :, :]\n",
    "#             label_slice = label[0, d, :, :]\n",
    "#             image_slice_uint8 = image_slice.astype(np.uint8)\n",
    "\n",
    "#             # save as numpy\n",
    "#             if not os.path.exists(f\"my_np_data/task{task}\"):\n",
    "#                 os.makedirs(f\"my_np_data/task{task}\")\n",
    "#                 os.makedirs(f\"my_np_data/task{task}/image\")\n",
    "#                 os.makedirs(f\"my_np_data/task{task}/label\")\n",
    "#                 os.makedirs(f\"my_np_data/task{task}/gray\")\n",
    "#             np.save(f'my_np_data/task{task}/image/image_{i}_{d}.npy', image_slice.cpu().numpy())\n",
    "#             np.save(f'my_np_data/task{task}/label/label_{i}_{d}.npy', label_slice.cpu().numpy())\n",
    "#             np.save(f'my_np_data/task{task}/gray/image_{i}_{d}.npy', image_slice_uint8)\n",
    "\n",
    "#         #     # 顯示圖像\n",
    "#         #     cv2.imshow('Image Slice', image_slice_uint8)\n",
    "#         #     cv2.imshow('Label Slice', label_slice.cpu().numpy())\n",
    "            \n",
    "#         #     # 等待並檢查是否有按鍵事件\n",
    "#         #     key = cv2.waitKey(0) & 0xFF\n",
    "            \n",
    "#         #     # 如果按下 'q' 鍵，則退出循環\n",
    "#         #     if key == ord('q'):\n",
    "#         #         break\n",
    "#         # # 如果按下 'q' 鍵，則退出循環\n",
    "#         # if key == ord('q'):\n",
    "#         #     break\n",
    "\n",
    "# # cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check npy data is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(457):\n",
    "#     img = np.load(f'my_np_data/task3/image/image_0_{i}.npy')\n",
    "#     gray = np.load(f'my_np_data/task3/gray/image_0_{i}.npy')\n",
    "#     label = np.load(f'my_np_data/task3/label/label_0_{i}.npy')\n",
    "#     cv2.imshow('image', img)\n",
    "#     cv2.imshow('label', label)\n",
    "#     cv2.imshow('gray', gray)\n",
    "#     # 等待並檢查是否有按鍵事件\n",
    "#     key = cv2.waitKey(0) & 0xFF\n",
    "\n",
    "#     # 如果按下 'q' 鍵，則退出循環\n",
    "#     if key == ord('q'):\n",
    "#         break\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# from collections import defaultdict\n",
    "\n",
    "# img_dir = 'my_np_data/task3/image'\n",
    "# pattern = re.compile(r'image_(\\d+)_(\\d+)\\.npy')\n",
    "\n",
    "# # 初始化一個defaultdict，預設值為0\n",
    "# slice_counts = defaultdict(int)\n",
    "\n",
    "# for filename in os.listdir(img_dir):\n",
    "#     match = pattern.match(filename)\n",
    "#     if match:\n",
    "#         I, _ = map(int, match.groups())\n",
    "#         # 每找到一個符合的檔案，就將對應的I的計數加一\n",
    "#         slice_counts[I] += 1\n",
    "\n",
    "# print(slice_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_of_data = {\n",
    "    3: { # task\n",
    "        0: 457, # slice of ith image\n",
    "        1: 503,\n",
    "        2: 521,\n",
    "        3: 496,\n",
    "        4: 615,\n",
    "        5: 417,\n",
    "        6: 667,\n",
    "        7: 667,\n",
    "        8: 667,\n",
    "        9: 552,\n",
    "        10: 570,\n",
    "        11: 512,\n",
    "        12: 456,\n",
    "        13: 665,\n",
    "    },\n",
    "    6: {\n",
    "        0: 607,\n",
    "        1: 546,\n",
    "        2: 532,\n",
    "        3: 510,\n",
    "        4: 481,\n",
    "        5: 631,\n",
    "        6: 470,\n",
    "        7: 557,\n",
    "        8: 519,\n",
    "        9: 506,\n",
    "        10: 494,\n",
    "        11: 449,\n",
    "        12: 532,\n",
    "        13: 506,\n",
    "    },\n",
    "    7: {\n",
    "        0: 586,\n",
    "        1: 556,\n",
    "        2: 490,\n",
    "        3: 488,\n",
    "        4: 620,\n",
    "        5: 463,\n",
    "        6: 522,\n",
    "        7: 550,\n",
    "        8: 547,\n",
    "        9: 471,\n",
    "        10: 547,\n",
    "        11: 485,\n",
    "        12: 536,\n",
    "        13: 450,\n",
    "    },\n",
    "    8: {\n",
    "        0: 587,\n",
    "        1: 609,\n",
    "        2: 619,\n",
    "        3: 604,\n",
    "        4: 572,\n",
    "        5: 540,\n",
    "        6: 551,\n",
    "        7: 450,\n",
    "        8: 504,\n",
    "        9: 469,\n",
    "        10: 499,\n",
    "        11: 450,\n",
    "        12: 499,\n",
    "        13: 463,\n",
    "    },\n",
    "    9: {\n",
    "        0: 523,\n",
    "        1: 486,\n",
    "        2: 528,\n",
    "        3: 492,\n",
    "        4: 485,\n",
    "        5: 403,\n",
    "        6: 477,\n",
    "        7: 515,\n",
    "        8: 515,\n",
    "        9: 604,\n",
    "        10: 508,\n",
    "        11: 631,\n",
    "        12: 583,\n",
    "        13: 631,\n",
    "    },\n",
    "    10: {\n",
    "        0: 462,\n",
    "        1: 512,\n",
    "        2: 597,\n",
    "        3: 530,\n",
    "        4: 470,\n",
    "        5: 540,\n",
    "        6: 508,\n",
    "        7: 503,\n",
    "        8: 531,\n",
    "        9: 495,\n",
    "        10: 515,\n",
    "        11: 513,\n",
    "        12: 612,\n",
    "        13: 531,\n",
    "    },\n",
    "}\n",
    "\n",
    "# 將nums_of_data轉成累加的形式\n",
    "for task, slices in nums_of_data.items():\n",
    "    for i in range(1, len(slices)):\n",
    "        slices[i] += slices[i - 1]\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, task_num):\n",
    "        self.img_dir = os.path.join(data_path, f\"task{task_num}\", \"image\")\n",
    "        self.label_dir = os.path.join(data_path, f\"task{task_num}\", \"label\")\n",
    "        self.task_num = task_num\n",
    "\n",
    "    def __len__(self):\n",
    "        return nums_of_data[self.task_num][13]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, d = self.find_image_and_slice(idx)\n",
    "        print(f\"{d}th slice of {i}th image\")\n",
    "        img_path = f'{self.img_dir}/image_{i}_{d}.npy'\n",
    "        label_path = f'{self.label_dir}/label_{i}_{d}.npy'\n",
    "        return torch.from_numpy(np.load(img_path)), torch.from_numpy(np.load(label_path))\n",
    "    \n",
    "    def find_image_and_slice(self, index):\n",
    "        for i in range(len(nums_of_data[self.task_num])):\n",
    "            if index < nums_of_data[self.task_num][i]:\n",
    "                slice_index = index - nums_of_data[self.task_num][i - 1] if i > 0 else index\n",
    "                return i, slice_index\n",
    "\n",
    "\n",
    "task = 3\n",
    "data_path = \"my_np_data\"\n",
    "dataset = MyDataset(data_path, task)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple, Type\n",
    "import math\n",
    "from einops import rearrange\n",
    "class Text2ImageTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int = 2,\n",
    "        embedding_dim: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        mlp_dim: int = 384,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.align_layers = nn.Sequential(\n",
    "            nn.Linear(512, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                Text2ImageAttentionBlock(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    activation=activation,\n",
    "                    attention_downsample_rate=attention_downsample_rate\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, image_embedding, text_embedding) -> Tuple[Tensor, Tensor]:\n",
    "        # image_embedding = [1,256,64,64]\n",
    "        # text_embedding = [1,512]\n",
    "        image_embedding = rearrange(image_embedding, 'b c h w -> b (h w) c')\n",
    "        text_embedding = self.align_layers(text_embedding).unsqueeze(1)\n",
    "        print(\"image_embedding supossed to be [1,4096,256]:\", image_embedding.size())\n",
    "        print(\"text_embedding supossed to be [1,1,256]:\", text_embedding.size())\n",
    "        for _, layer in enumerate(self.layers): # 2個block\n",
    "            image_embedding, text_embedding = layer(\n",
    "                image_embedding,\n",
    "                text_embedding\n",
    "            )\n",
    "\n",
    "        # 此時image_embedding主要包含了圖像本身的資訊，而text_embedding則包含了圖像和文本之間的關聯資訊。\n",
    "        return image_embedding, text_embedding\n",
    "    \n",
    "class Text2ImageAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        mlp_dim: int = 384,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = Attention(embedding_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.cross_attn = Attention(embedding_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "        # self.global_query = nn.parameter.Parameter(data=0.1 * torch.randn(1, 10, embedding_dim)) # [1,10,256]的可訓練參數(隨機產生的正態分布*0.1)\n",
    "\n",
    "    def forward(self, image_embedding, text_embedding) -> Tensor:\n",
    "        # self attention\n",
    "        self_out = self.self_attn(q=image_embedding, k=image_embedding, v=image_embedding)\n",
    "        self_out = self.norm1(image_embedding + self_out) # skip connection\n",
    "        print(\"self_out\", self_out.size())\n",
    "\n",
    "        # cross attention\n",
    "        cross_out = self.cross_attn(q=self_out, k=text_embedding, v=text_embedding)\n",
    "        cross_out = self.norm2(self_out + cross_out) # skip connection\n",
    "        print(\"cross_out\", self_out.size())\n",
    "\n",
    "        # MLP block\n",
    "        mlp_out = self.mlp(cross_out)\n",
    "        mlp_out = self.norm3(cross_out + mlp_out) # skip connection\n",
    "        print(\"mlp_out\", mlp_out.size())\n",
    "\n",
    "        return self_out, mlp_out\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        downsample_rate: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate # 全聯接層的縮放大小\n",
    "        self.num_heads = num_heads\n",
    "        assert self.internal_dim % num_heads == 0, \"num_heads 必須整除 embedding_dim\"\n",
    "\n",
    "        # MLP dim => internal_dim\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "\n",
    "        # MLP internal_dim => dim\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        b, n, c = x.shape\n",
    "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, c_per_head = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        q = self.q_proj(q) # MLP\n",
    "        k = self.k_proj(k) # MLP\n",
    "        v = self.v_proj(v) # MLP\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        # Attention\n",
    "        _, _, _, c_per_head = q.shape\n",
    "        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n",
    "        attn = attn / math.sqrt(c_per_head) # 這種縮放操作可以防止當特徵數值很大時，點積的結果過大導致的梯度爆炸問題。\n",
    "        attn = torch.softmax(attn, dim=-1) # 在最後一個維度做，對應於一個Q對所有K的相似度\n",
    "\n",
    "        # Get output\n",
    "        out = attn @ v\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "    \n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        mlp_dim: int,\n",
    "        act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dim),\n",
    "            act(),\n",
    "            nn.Linear(mlp_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing purposes\n",
    "image_embedding = torch.randn(1,256,64,64)\n",
    "text_embedding = torch.randn(1,512)\n",
    "trans = Text2ImageTransformer()\n",
    "a, b = trans(image_embedding, text_embedding)\n",
    "print(a.size(),b.size())\n",
    "# TODO: postion embedding\n",
    "# TODO: is GLOBAL QUERY needed?\n",
    "# TODO: decoder study and implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "lora.Linear(16, 24, r=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextSAM, self).__init__()\n",
    "        self.image_enc = sam.image_encoder\n",
    "        self.clip = clip_model\n",
    "        self.linear = nn.Linear(256,512)\n",
    "        self.transformer = nn.Transformer(d_model=512, nhead=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image_enc(x)\n",
    "        x = x.view(1,256,-1).permute(0,2,1)\n",
    "        x = self.linear(x)\n",
    "        xt = self.clip.encode_text(clip.tokenize([\"lung\"]).to(device))\n",
    "        xt = xt.unsqueeze(0)\n",
    "        # x = x.permute(1,0,2)\n",
    "        # xt = xt.permute(1,0,2)\n",
    "        print(x.size())\n",
    "        print(xt.size())\n",
    "\n",
    "        # x = self.transformer(x,xt)\n",
    "        return x\n",
    "\n",
    "model = TextSAM()\n",
    "for batch in dataloader:\n",
    "    images, labels = batch\n",
    "    images = sam.preprocess(images)\n",
    "    images = images.unsqueeze(0)\n",
    "    print(images.size()) # [1, 3, 1024, 1024]\n",
    "    model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "num_epochs = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
